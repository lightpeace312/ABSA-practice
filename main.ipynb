{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# file: atae-lstm\n",
    "# author: songyouwei <youwei0314@gmail.com>\n",
    "# Copyright (C) 2018. All Rights Reserved.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import math\n",
    "import os\n",
    "from model import ATAE_LSTM, AOA\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, ABSADataset\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Arguments for restaurant,ATAE-LSTM\n",
    "# opt = Namespace(\n",
    "#     model_name=\"atae_lstm\",\n",
    "#     dataset='restaurant',#twitter,laptop\n",
    "#     seed=1234,\n",
    "#     optimizer = 'adam',\n",
    "#     initializer = 'xavier_uniform_',\n",
    "#     log_step = 5,\n",
    "#     logdir = 'log',\n",
    "#     embed_dim = 200,\n",
    "#     hidden_dim = 300,\n",
    "#     max_seq_len = 80,\n",
    "#     polarities_dim = 3,\n",
    "#     hops = 3,\n",
    "#     device = None,\n",
    "#     learning_rate = 0.001,\n",
    "#     batch_size = 128,\n",
    "#     l2reg = 0.00001,\n",
    "#     num_epoch = 20,\n",
    "#     dropout = 0,\n",
    "# )\n",
    "# dataset_files = {\n",
    "#     'twitter': {\n",
    "#         'train': './datasets/acl-14-short-data/train.raw',\n",
    "#         'test': './datasets/acl-14-short-data/test.raw'\n",
    "#     },\n",
    "#     'restaurant': {\n",
    "#         'train': './datasets/semeval14/Restaurants_Train.xml.seg',\n",
    "#         'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'\n",
    "#     },\n",
    "#     'laptop': {\n",
    "#         'train': './datasets/semeval14/Laptops_Train.xml.seg',\n",
    "#         'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'\n",
    "#     }\n",
    "# }\n",
    "# input_colses = {\n",
    "#         'atae_lstm': ['text_raw_indices', 'aspect_indices']\n",
    "#         'aoa': ['text_raw_indices', 'aspect_indices']\n",
    "#     }\n",
    "# initializers = {\n",
    "#         'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "#         'xavier_normal_': torch.nn.init.xavier_normal,\n",
    "#         'orthogonal_': torch.nn.init.orthogonal_,\n",
    "#     }\n",
    "# optimizers = {\n",
    "#         'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
    "#         'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "#         'adam': torch.optim.Adam,  # default lr=0.001\n",
    "#         'adamax': torch.optim.Adamax,  # default lr=0.002\n",
    "#         'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "#         'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
    "#         'sgd': torch.optim.SGD,\n",
    "#     }\n",
    "# opt.inputs_cols = input_colses['atae_lstm']\n",
    "# opt.dataset_file = dataset_files[opt.dataset]\n",
    "# opt.initializer = initializers[opt.initializer]\n",
    "# opt.optimizer = optimizers[opt.optimizer]\n",
    "# # opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# opt.device = torch.device('cpu')\n",
    "\n",
    "# opt.model_class=ATAE_LSTM\n",
    "# # Set seed for reproducability\n",
    "# np.random.seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for restaurant,AOA\n",
    "opt = Namespace(\n",
    "    model_name=\"aoa\",\n",
    "    dataset='restaurant',#twitter,laptop\n",
    "    seed=1234,\n",
    "    optimizer = 'adam',\n",
    "    initializer = 'xavier_uniform_',\n",
    "    log_step = 5,\n",
    "    logdir = 'log',\n",
    "    embed_dim = 200,\n",
    "    hidden_dim = 300,\n",
    "    max_seq_len = 80,\n",
    "    polarities_dim = 3,\n",
    "    hops = 3,\n",
    "    device = None,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 128,\n",
    "    l2reg = 0.0001,#0.00001\n",
    "    num_epoch = 20,\n",
    "    dropout = 0.2,\n",
    ")\n",
    "dataset_files = {\n",
    "    'twitter': {\n",
    "        'train': './datasets/acl-14-short-data/train.raw',\n",
    "        'test': './datasets/acl-14-short-data/test.raw'\n",
    "    },\n",
    "    'restaurant': {\n",
    "        'train': './datasets/semeval14/Restaurants_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'\n",
    "    },\n",
    "    'laptop': {\n",
    "        'train': './datasets/semeval14/Laptops_Train.xml.seg',\n",
    "        'test': './datasets/semeval14/Laptops_Test_Gold.xml.seg'\n",
    "    }\n",
    "}\n",
    "input_colses = {\n",
    "        'atae_lstm': ['text_raw_indices', 'aspect_indices'],\n",
    "        'aoa': ['text_raw_indices', 'aspect_indices']\n",
    "    }\n",
    "initializers = {\n",
    "        'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "        'xavier_normal_': torch.nn.init.xavier_normal,\n",
    "        'orthogonal_': torch.nn.init.orthogonal_,\n",
    "    }\n",
    "optimizers = {\n",
    "        'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
    "        'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "        'adam': torch.optim.Adam,  # default lr=0.001\n",
    "        'adamax': torch.optim.Adamax,  # default lr=0.002\n",
    "        'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "        'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
    "        'sgd': torch.optim.SGD,\n",
    "    }\n",
    "opt.inputs_cols = input_colses['atae_lstm']\n",
    "opt.dataset_file = dataset_files[opt.dataset]\n",
    "opt.initializer = initializers[opt.initializer]\n",
    "opt.optimizer = optimizers[opt.optimizer]\n",
    "# opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "opt.device = torch.device('cpu')\n",
    "\n",
    "opt.model_class=AOA\n",
    "# Set seed for reproducability\n",
    "np.random.seed(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "        \n",
    "        tokenizer = build_tokenizer(\n",
    "            fnames=[opt.dataset_file['train'], opt.dataset_file['test']],\n",
    "            max_seq_len=opt.max_seq_len,\n",
    "            dat_fname='{0}_tokenizer.dat'.format(opt.dataset))\n",
    "        embedding_matrix = build_embedding_matrix(\n",
    "            word2idx=tokenizer.word2idx,\n",
    "            embed_dim=opt.embed_dim,\n",
    "            em_fname='{0}_{1}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset),\n",
    "            ev_fpath='../../../data/embeddings/glove.twitter.27B/')\n",
    "        self.model = opt.model_class(embedding_matrix, opt).to(opt.device)\n",
    "\n",
    "        trainset = ABSADataset(opt.dataset_file['train'], tokenizer)\n",
    "        testset = ABSADataset(opt.dataset_file['test'], tokenizer)\n",
    "        self.train_data_loader = DataLoader(dataset=trainset, batch_size=opt.batch_size, shuffle=True)\n",
    "        self.test_data_loader = DataLoader(dataset=testset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "        if opt.device.type == 'cuda':\n",
    "            print(\"cuda memory allocated:\", torch.cuda.memory_allocated(device=opt.device.index))\n",
    "        self._print_args()\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape))\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        print('n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
    "        print('> training arguments:')\n",
    "        for arg in vars(self.opt):\n",
    "            print('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for child in self.model.children():\n",
    "#             if type(child) != BertModel:  # skip bert params (with unfreezed bert)\n",
    "            for p in child.parameters():\n",
    "                if p.requires_grad:\n",
    "                    if len(p.shape) > 1:\n",
    "                        self.opt.initializer(p)\n",
    "                    else:\n",
    "                        stdv = 1. / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def _train(self, criterion, optimizer, max_test_acc_overall=0):\n",
    "        writer = SummaryWriter(log_dir=self.opt.logdir)\n",
    "        max_test_acc = 0\n",
    "        max_f1 = 0\n",
    "        global_step = 0\n",
    "        for epoch in range(self.opt.num_epoch):\n",
    "            print('>' * 100)\n",
    "            print('epoch: ', epoch)\n",
    "            n_correct, n_total = 0, 0\n",
    "            for i_batch, sample_batched in enumerate(self.train_data_loader):\n",
    "                global_step += 1\n",
    "\n",
    "                # switch model to training mode, clear gradient accumulators\n",
    "                self.model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs = [sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
    "                outputs = self.model(inputs)\n",
    "                targets = sample_batched['polarity'].to(self.opt.device)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                    n_total += len(outputs)\n",
    "                    train_acc = n_correct / n_total\n",
    "\n",
    "                    test_acc, f1 = self._evaluate_acc_f1()\n",
    "                    if test_acc > max_test_acc:\n",
    "                        max_test_acc = test_acc\n",
    "                        if test_acc > max_test_acc_overall:\n",
    "                            if not os.path.exists('state_dict'):\n",
    "                                os.mkdir('state_dict')\n",
    "                            path = 'state_dict/{0}_{1}_acc{2}'.format(self.opt.model_name, self.opt.dataset, round(test_acc, 4))\n",
    "                            torch.save(self.model.state_dict(), path)\n",
    "                            print('>> saved: ' + path)\n",
    "                    if f1 > max_f1:\n",
    "                        max_f1 = f1\n",
    "\n",
    "                    writer.add_scalar('loss', loss, global_step)\n",
    "                    writer.add_scalar('acc', train_acc, global_step)\n",
    "                    writer.add_scalar('test_acc', test_acc, global_step)\n",
    "                    print('loss: {:.4f}, acc: {:.4f}, test_acc: {:.4f}, f1: {:.4f}'.format(loss.item(), train_acc, test_acc, f1))\n",
    "\n",
    "        writer.close()\n",
    "        return max_test_acc, max_f1\n",
    "\n",
    "    def _evaluate_acc_f1(self):\n",
    "        # switch model to evaluation mode\n",
    "        self.model.eval()\n",
    "        n_test_correct, n_test_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        with torch.no_grad():\n",
    "            for t_batch, t_sample_batched in enumerate(self.test_data_loader):\n",
    "                t_inputs = [t_sample_batched[col].to(opt.device) for col in self.opt.inputs_cols]\n",
    "                t_targets = t_sample_batched['polarity'].to(opt.device)\n",
    "                t_outputs = self.model(t_inputs)\n",
    "\n",
    "                n_test_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_test_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "        test_acc = n_test_correct / n_test_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "        return test_acc, f1\n",
    "\n",
    "    def run(self, repeats=1):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = self.opt.optimizer(_params, lr=self.opt.learning_rate, weight_decay=self.opt.l2reg)\n",
    "\n",
    "        max_test_acc_overall = 0\n",
    "        max_f1_overall = 0\n",
    "        for i in range(repeats):\n",
    "            print('repeat: ', i)\n",
    "            self._reset_params()\n",
    "            max_test_acc, max_f1 = self._train(criterion, optimizer, max_test_acc_overall=max_test_acc_overall)\n",
    "            print('max_test_acc: {0}     max_f1: {1}'.format(max_test_acc, max_f1))\n",
    "            max_test_acc_overall = max(max_test_acc, max_test_acc_overall)\n",
    "            max_f1_overall = max(max_f1, max_f1_overall)\n",
    "            print('#' * 100)\n",
    "        print(\"max_test_acc_overall:\", max_test_acc_overall)\n",
    "        print(\"max_f1_overall:\", max_f1_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: restaurant_tokenizer.dat\n",
      "loading word vectors...\n",
      "building embedding_matrix: 200_restaurant_embedding_matrix.dat\n",
      "n_trainable_params: 2411403, n_nontrainable_params: 917000\n",
      "> training arguments:\n",
      ">>> model_name: aoa\n",
      ">>> dataset: restaurant\n",
      ">>> seed: 1234\n",
      ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
      ">>> initializer: <function xavier_uniform_ at 0x0000025139DD71E0>\n",
      ">>> log_step: 5\n",
      ">>> logdir: log\n",
      ">>> embed_dim: 200\n",
      ">>> hidden_dim: 300\n",
      ">>> max_seq_len: 80\n",
      ">>> polarities_dim: 3\n",
      ">>> hops: 3\n",
      ">>> device: cpu\n",
      ">>> learning_rate: 0.001\n",
      ">>> batch_size: 128\n",
      ">>> l2reg: 0.0001\n",
      ">>> num_epoch: 20\n",
      ">>> dropout: 0.2\n",
      ">>> inputs_cols: ['text_raw_indices', 'aspect_indices']\n",
      ">>> dataset_file: {'train': './datasets/semeval14/Restaurants_Train.xml.seg', 'test': './datasets/semeval14/Restaurants_Test_Gold.xml.seg'}\n",
      ">>> model_class: <class 'model.AOA'>\n"
     ]
    }
   ],
   "source": [
    "ins = Instructor(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat:  0\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> saved: state_dict/aoa_restaurant_acc0.65\n",
      "loss: 0.9633, acc: 0.6406, test_acc: 0.6500, f1: 0.2626\n",
      ">> saved: state_dict/aoa_restaurant_acc0.6518\n",
      "loss: 1.0315, acc: 0.5859, test_acc: 0.6518, f1: 0.2729\n",
      ">> saved: state_dict/aoa_restaurant_acc0.6554\n",
      "loss: 0.9464, acc: 0.5859, test_acc: 0.6554, f1: 0.2835\n",
      ">> saved: state_dict/aoa_restaurant_acc0.6643\n",
      "loss: 0.7981, acc: 0.6172, test_acc: 0.6643, f1: 0.3219\n",
      ">> saved: state_dict/aoa_restaurant_acc0.6732\n",
      "loss: 0.8998, acc: 0.6094, test_acc: 0.6732, f1: 0.4499\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 0.8280, acc: 0.6719, test_acc: 0.6687, f1: 0.5132\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7018\n",
      "loss: 1.1377, acc: 0.6055, test_acc: 0.7018, f1: 0.4523\n",
      "loss: 0.8093, acc: 0.6302, test_acc: 0.7000, f1: 0.4896\n",
      "loss: 0.7451, acc: 0.6543, test_acc: 0.6964, f1: 0.4282\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7036\n",
      "loss: 0.7342, acc: 0.6609, test_acc: 0.7036, f1: 0.4663\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7259\n",
      "loss: 0.7510, acc: 0.6693, test_acc: 0.7259, f1: 0.5916\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7321\n",
      "loss: 0.6991, acc: 0.7031, test_acc: 0.7321, f1: 0.5374\n",
      "loss: 0.6816, acc: 0.6875, test_acc: 0.7232, f1: 0.5108\n",
      "loss: 0.7062, acc: 0.6927, test_acc: 0.7188, f1: 0.5695\n",
      "loss: 0.7666, acc: 0.6973, test_acc: 0.7125, f1: 0.5641\n",
      "loss: 0.7191, acc: 0.6984, test_acc: 0.7214, f1: 0.5276\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7393\n",
      "loss: 0.7401, acc: 0.6979, test_acc: 0.7393, f1: 0.5756\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 0.6065, acc: 0.7500, test_acc: 0.7321, f1: 0.5873\n",
      "loss: 0.7151, acc: 0.7227, test_acc: 0.7188, f1: 0.5124\n",
      ">> saved: state_dict/aoa_restaurant_acc0.75\n",
      "loss: 0.6670, acc: 0.7135, test_acc: 0.7500, f1: 0.6218\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7509\n",
      "loss: 0.6802, acc: 0.7129, test_acc: 0.7509, f1: 0.6217\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7518\n",
      "loss: 0.7380, acc: 0.7109, test_acc: 0.7518, f1: 0.6102\n",
      "loss: 0.6437, acc: 0.7135, test_acc: 0.7420, f1: 0.5982\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.6005, acc: 0.7891, test_acc: 0.7411, f1: 0.6114\n",
      "loss: 0.6274, acc: 0.7539, test_acc: 0.7455, f1: 0.5587\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7652\n",
      "loss: 0.4962, acc: 0.7682, test_acc: 0.7652, f1: 0.6372\n",
      "loss: 0.6571, acc: 0.7520, test_acc: 0.7509, f1: 0.5992\n",
      "loss: 0.6482, acc: 0.7453, test_acc: 0.7455, f1: 0.5721\n",
      "loss: 0.6475, acc: 0.7440, test_acc: 0.7527, f1: 0.6146\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.5539, acc: 0.7578, test_acc: 0.7580, f1: 0.6145\n",
      "loss: 0.5058, acc: 0.7812, test_acc: 0.7482, f1: 0.6138\n",
      "loss: 0.4421, acc: 0.7865, test_acc: 0.7518, f1: 0.6048\n",
      "loss: 0.5609, acc: 0.7852, test_acc: 0.7607, f1: 0.6244\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7688\n",
      "loss: 0.4464, acc: 0.7937, test_acc: 0.7688, f1: 0.6528\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.4769, acc: 0.8359, test_acc: 0.7536, f1: 0.5901\n",
      "loss: 0.4957, acc: 0.8438, test_acc: 0.7509, f1: 0.6007\n",
      "loss: 0.4630, acc: 0.8385, test_acc: 0.7509, f1: 0.6398\n",
      "loss: 0.4841, acc: 0.8262, test_acc: 0.7527, f1: 0.6095\n",
      "loss: 0.5179, acc: 0.8250, test_acc: 0.7527, f1: 0.5968\n",
      "loss: 0.5312, acc: 0.8229, test_acc: 0.7357, f1: 0.6519\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n",
      "loss: 0.4509, acc: 0.7969, test_acc: 0.7643, f1: 0.6252\n",
      "loss: 0.5435, acc: 0.7734, test_acc: 0.7661, f1: 0.6310\n",
      "loss: 0.4425, acc: 0.7943, test_acc: 0.7607, f1: 0.6595\n",
      "loss: 0.5040, acc: 0.7988, test_acc: 0.7616, f1: 0.6195\n",
      "loss: 0.3829, acc: 0.8141, test_acc: 0.7643, f1: 0.6556\n",
      "loss: 0.5245, acc: 0.8060, test_acc: 0.7679, f1: 0.6430\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  8\n",
      "loss: 0.4884, acc: 0.8047, test_acc: 0.7580, f1: 0.6249\n",
      "loss: 0.4342, acc: 0.8203, test_acc: 0.7616, f1: 0.6394\n",
      "loss: 0.3958, acc: 0.8333, test_acc: 0.7616, f1: 0.6271\n",
      "loss: 0.4236, acc: 0.8301, test_acc: 0.7670, f1: 0.6488\n",
      "loss: 0.4177, acc: 0.8297, test_acc: 0.7661, f1: 0.6546\n",
      "loss: 0.3129, acc: 0.8411, test_acc: 0.7527, f1: 0.6012\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  9\n",
      "loss: 0.2606, acc: 0.9375, test_acc: 0.7580, f1: 0.6477\n",
      "loss: 0.3921, acc: 0.8945, test_acc: 0.7545, f1: 0.6328\n",
      "loss: 0.2925, acc: 0.8932, test_acc: 0.7429, f1: 0.6148\n",
      "loss: 0.3796, acc: 0.8848, test_acc: 0.7554, f1: 0.6097\n",
      "loss: 0.3864, acc: 0.8797, test_acc: 0.7634, f1: 0.6518\n",
      "loss: 0.3086, acc: 0.8780, test_acc: 0.7670, f1: 0.6373\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  10\n",
      "loss: 0.3174, acc: 0.8906, test_acc: 0.7598, f1: 0.6105\n",
      "loss: 0.4075, acc: 0.8633, test_acc: 0.7571, f1: 0.6404\n",
      "loss: 0.3857, acc: 0.8568, test_acc: 0.7616, f1: 0.6318\n",
      "loss: 0.3427, acc: 0.8633, test_acc: 0.7652, f1: 0.6583\n",
      "loss: 0.3722, acc: 0.8609, test_acc: 0.7679, f1: 0.6654\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  11\n",
      "loss: 0.2511, acc: 0.9141, test_acc: 0.7438, f1: 0.6233\n",
      "loss: 0.4701, acc: 0.8789, test_acc: 0.7598, f1: 0.6487\n",
      "loss: 0.3932, acc: 0.8724, test_acc: 0.7571, f1: 0.6455\n",
      "loss: 0.3003, acc: 0.8730, test_acc: 0.7598, f1: 0.6115\n",
      "loss: 0.4145, acc: 0.8719, test_acc: 0.7634, f1: 0.6309\n",
      "loss: 0.2956, acc: 0.8711, test_acc: 0.7598, f1: 0.6395\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  12\n",
      "loss: 0.2254, acc: 0.9297, test_acc: 0.7589, f1: 0.6126\n",
      "loss: 0.3479, acc: 0.8984, test_acc: 0.7661, f1: 0.6503\n",
      "loss: 0.2483, acc: 0.9115, test_acc: 0.7607, f1: 0.6495\n",
      "loss: 0.2826, acc: 0.9141, test_acc: 0.7589, f1: 0.6192\n",
      "loss: 0.3188, acc: 0.9000, test_acc: 0.7679, f1: 0.6574\n",
      "loss: 0.3235, acc: 0.8906, test_acc: 0.7643, f1: 0.6564\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  13\n",
      "loss: 0.2328, acc: 0.8906, test_acc: 0.7562, f1: 0.6227\n",
      "loss: 0.2483, acc: 0.9023, test_acc: 0.7554, f1: 0.6494\n",
      "loss: 0.2360, acc: 0.9115, test_acc: 0.7598, f1: 0.6424\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7714\n",
      "loss: 0.2496, acc: 0.9082, test_acc: 0.7714, f1: 0.6529\n",
      "loss: 0.2972, acc: 0.9094, test_acc: 0.7705, f1: 0.6507\n",
      "loss: 0.2313, acc: 0.9076, test_acc: 0.7616, f1: 0.6444\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  14\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7759\n",
      "loss: 0.2257, acc: 0.8906, test_acc: 0.7759, f1: 0.6533\n",
      "loss: 0.2537, acc: 0.8906, test_acc: 0.7759, f1: 0.6718\n",
      "loss: 0.2279, acc: 0.9036, test_acc: 0.7598, f1: 0.6504\n",
      "loss: 0.2086, acc: 0.9082, test_acc: 0.7634, f1: 0.6439\n",
      "loss: 0.2131, acc: 0.9062, test_acc: 0.7598, f1: 0.6596\n",
      "loss: 0.1257, acc: 0.9081, test_acc: 0.7589, f1: 0.6375\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  15\n",
      "loss: 0.1560, acc: 0.9453, test_acc: 0.7455, f1: 0.6272\n",
      "loss: 0.2139, acc: 0.9297, test_acc: 0.7527, f1: 0.6280\n",
      "loss: 0.1645, acc: 0.9297, test_acc: 0.7580, f1: 0.6324\n",
      "loss: 0.2679, acc: 0.9258, test_acc: 0.7527, f1: 0.6378\n",
      "loss: 0.2204, acc: 0.9203, test_acc: 0.7580, f1: 0.6301\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  16\n",
      "loss: 0.1709, acc: 0.9375, test_acc: 0.7580, f1: 0.6743\n",
      "loss: 0.1307, acc: 0.9531, test_acc: 0.7634, f1: 0.6357\n",
      "loss: 0.2160, acc: 0.9375, test_acc: 0.7598, f1: 0.6380\n",
      "loss: 0.2365, acc: 0.9258, test_acc: 0.7464, f1: 0.6361\n",
      "loss: 0.2653, acc: 0.9234, test_acc: 0.7500, f1: 0.6141\n",
      "loss: 0.1797, acc: 0.9232, test_acc: 0.7482, f1: 0.6314\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  17\n",
      "loss: 0.1651, acc: 0.9375, test_acc: 0.7536, f1: 0.6256\n",
      "loss: 0.1938, acc: 0.9453, test_acc: 0.7598, f1: 0.6288\n",
      "loss: 0.1522, acc: 0.9453, test_acc: 0.7696, f1: 0.6557\n",
      "loss: 0.2669, acc: 0.9336, test_acc: 0.7643, f1: 0.6394\n",
      "loss: 0.2343, acc: 0.9359, test_acc: 0.7688, f1: 0.6668\n",
      "loss: 0.1401, acc: 0.9375, test_acc: 0.7607, f1: 0.6344\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  18\n",
      "loss: 0.1232, acc: 0.9609, test_acc: 0.7598, f1: 0.6532\n",
      "loss: 0.1739, acc: 0.9492, test_acc: 0.7571, f1: 0.6400\n",
      "loss: 0.2071, acc: 0.9401, test_acc: 0.7598, f1: 0.6460\n",
      "loss: 0.1509, acc: 0.9375, test_acc: 0.7589, f1: 0.6445\n",
      "loss: 0.0757, acc: 0.9469, test_acc: 0.7500, f1: 0.6081\n",
      "loss: 0.1405, acc: 0.9479, test_acc: 0.7446, f1: 0.6394\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  19\n",
      "loss: 0.1297, acc: 0.9453, test_acc: 0.7509, f1: 0.6354\n",
      "loss: 0.1718, acc: 0.9414, test_acc: 0.7509, f1: 0.6211\n",
      "loss: 0.1144, acc: 0.9531, test_acc: 0.7509, f1: 0.6293\n",
      "loss: 0.0840, acc: 0.9590, test_acc: 0.7536, f1: 0.6284\n",
      "loss: 0.1270, acc: 0.9609, test_acc: 0.7554, f1: 0.6312\n",
      "loss: 0.0932, acc: 0.9608, test_acc: 0.7580, f1: 0.6501\n",
      "max_test_acc: 0.7758928571428572     max_f1: 0.6742675729970576\n",
      "####################################################################################################\n",
      "repeat:  1\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n",
      "loss: 1.0303, acc: 0.6562, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 1.0009, acc: 0.6289, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9983, acc: 0.6146, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 1.0453, acc: 0.6133, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 1.0282, acc: 0.5969, test_acc: 0.6500, f1: 0.2626\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 0.9562, acc: 0.6328, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9466, acc: 0.6250, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9403, acc: 0.6120, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9188, acc: 0.6133, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.8063, acc: 0.6188, test_acc: 0.6732, f1: 0.3457\n",
      "loss: 0.8320, acc: 0.6328, test_acc: 0.6911, f1: 0.4422\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      "loss: 0.8986, acc: 0.5547, test_acc: 0.6723, f1: 0.4428\n",
      "loss: 0.9201, acc: 0.5781, test_acc: 0.7036, f1: 0.4467\n",
      "loss: 0.8501, acc: 0.6016, test_acc: 0.7089, f1: 0.4634\n",
      "loss: 0.8100, acc: 0.6094, test_acc: 0.6634, f1: 0.3137\n",
      "loss: 0.8181, acc: 0.6297, test_acc: 0.7009, f1: 0.4681\n",
      "loss: 0.8461, acc: 0.6380, test_acc: 0.7063, f1: 0.4605\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 0.8347, acc: 0.6172, test_acc: 0.7000, f1: 0.4438\n",
      "loss: 0.6333, acc: 0.6758, test_acc: 0.7152, f1: 0.4735\n",
      "loss: 0.8053, acc: 0.6823, test_acc: 0.7179, f1: 0.4960\n",
      "loss: 0.6747, acc: 0.6973, test_acc: 0.7259, f1: 0.5202\n",
      "loss: 0.7667, acc: 0.6969, test_acc: 0.7223, f1: 0.5044\n",
      "loss: 0.7135, acc: 0.7031, test_acc: 0.7304, f1: 0.5237\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.6149, acc: 0.7266, test_acc: 0.7277, f1: 0.5239\n",
      "loss: 0.6467, acc: 0.7227, test_acc: 0.7366, f1: 0.5311\n",
      "loss: 0.6358, acc: 0.7292, test_acc: 0.7321, f1: 0.5180\n",
      "loss: 0.6959, acc: 0.7168, test_acc: 0.7375, f1: 0.5312\n",
      "loss: 0.6684, acc: 0.7203, test_acc: 0.7402, f1: 0.5552\n",
      "loss: 0.7436, acc: 0.7169, test_acc: 0.7482, f1: 0.5741\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.6437, acc: 0.7031, test_acc: 0.7348, f1: 0.5946\n",
      "loss: 0.5964, acc: 0.7383, test_acc: 0.7420, f1: 0.5246\n",
      "loss: 0.6524, acc: 0.7266, test_acc: 0.7384, f1: 0.5525\n",
      "loss: 0.6617, acc: 0.7227, test_acc: 0.7473, f1: 0.5708\n",
      "loss: 0.7640, acc: 0.7172, test_acc: 0.7554, f1: 0.5795\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.6060, acc: 0.7109, test_acc: 0.7589, f1: 0.6482\n",
      "loss: 0.6281, acc: 0.6992, test_acc: 0.7518, f1: 0.5683\n",
      "loss: 0.6924, acc: 0.7083, test_acc: 0.7670, f1: 0.5993\n",
      "loss: 0.6275, acc: 0.7129, test_acc: 0.7634, f1: 0.6664\n",
      "loss: 0.6322, acc: 0.7203, test_acc: 0.7491, f1: 0.5629\n",
      "loss: 0.7034, acc: 0.7214, test_acc: 0.7446, f1: 0.5534\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n",
      "loss: 0.5436, acc: 0.7656, test_acc: 0.7527, f1: 0.6309\n",
      "loss: 0.4758, acc: 0.7930, test_acc: 0.7571, f1: 0.5971\n",
      "loss: 0.5678, acc: 0.7839, test_acc: 0.7473, f1: 0.5520\n",
      "loss: 0.5251, acc: 0.7812, test_acc: 0.7420, f1: 0.5694\n",
      "loss: 0.5099, acc: 0.7828, test_acc: 0.7554, f1: 0.6005\n",
      "loss: 0.6049, acc: 0.7747, test_acc: 0.7625, f1: 0.6260\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  8\n",
      "loss: 0.5155, acc: 0.7969, test_acc: 0.7679, f1: 0.6340\n",
      "loss: 0.5020, acc: 0.8086, test_acc: 0.7580, f1: 0.6199\n",
      "loss: 0.5092, acc: 0.8073, test_acc: 0.7670, f1: 0.6440\n",
      "loss: 0.4716, acc: 0.8145, test_acc: 0.7518, f1: 0.6221\n",
      "loss: 0.4471, acc: 0.8109, test_acc: 0.7580, f1: 0.5880\n",
      "loss: 0.5413, acc: 0.8086, test_acc: 0.7598, f1: 0.6333\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  9\n",
      "loss: 0.5033, acc: 0.7969, test_acc: 0.7652, f1: 0.6402\n",
      "loss: 0.3719, acc: 0.8281, test_acc: 0.7473, f1: 0.5775\n",
      "loss: 0.4809, acc: 0.8151, test_acc: 0.7679, f1: 0.6373\n",
      "loss: 0.5049, acc: 0.8086, test_acc: 0.7679, f1: 0.6304\n",
      "loss: 0.4801, acc: 0.8063, test_acc: 0.7705, f1: 0.6409\n",
      "loss: 0.4210, acc: 0.8087, test_acc: 0.7679, f1: 0.6700\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  10\n",
      "loss: 0.4190, acc: 0.8359, test_acc: 0.7652, f1: 0.6373\n",
      "loss: 0.4207, acc: 0.8320, test_acc: 0.7554, f1: 0.6222\n",
      "loss: 0.3603, acc: 0.8516, test_acc: 0.7696, f1: 0.6532\n",
      "loss: 0.5153, acc: 0.8262, test_acc: 0.7750, f1: 0.6602\n",
      "loss: 0.3584, acc: 0.8328, test_acc: 0.7661, f1: 0.6408\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  11\n",
      "loss: 0.4833, acc: 0.7812, test_acc: 0.7598, f1: 0.6394\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7768\n",
      "loss: 0.4018, acc: 0.8125, test_acc: 0.7768, f1: 0.6562\n",
      "loss: 0.3965, acc: 0.8099, test_acc: 0.7714, f1: 0.6579\n",
      "loss: 0.3337, acc: 0.8281, test_acc: 0.7714, f1: 0.6554\n",
      "loss: 0.4285, acc: 0.8250, test_acc: 0.7500, f1: 0.6143\n",
      "loss: 0.2864, acc: 0.8307, test_acc: 0.7661, f1: 0.6445\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  12\n",
      "loss: 0.3901, acc: 0.8359, test_acc: 0.7589, f1: 0.6326\n",
      "loss: 0.3618, acc: 0.8320, test_acc: 0.7714, f1: 0.6531\n",
      "loss: 0.4153, acc: 0.8255, test_acc: 0.7732, f1: 0.6629\n",
      "loss: 0.3407, acc: 0.8379, test_acc: 0.7670, f1: 0.6440\n",
      "loss: 0.3191, acc: 0.8438, test_acc: 0.7625, f1: 0.6504\n",
      "loss: 0.3671, acc: 0.8438, test_acc: 0.7616, f1: 0.6498\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  13\n",
      "loss: 0.3611, acc: 0.8828, test_acc: 0.7643, f1: 0.6634\n",
      "loss: 0.2470, acc: 0.9023, test_acc: 0.7589, f1: 0.6323\n",
      "loss: 0.3331, acc: 0.9010, test_acc: 0.7714, f1: 0.6776\n",
      "loss: 0.3935, acc: 0.8848, test_acc: 0.7545, f1: 0.6140\n",
      "loss: 0.2881, acc: 0.8922, test_acc: 0.7679, f1: 0.6700\n",
      "loss: 0.2762, acc: 0.8919, test_acc: 0.7661, f1: 0.6390\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  14\n",
      "loss: 0.3276, acc: 0.8672, test_acc: 0.7562, f1: 0.6210\n",
      "loss: 0.2794, acc: 0.8711, test_acc: 0.7491, f1: 0.6462\n",
      "loss: 0.2853, acc: 0.8776, test_acc: 0.7643, f1: 0.6356\n",
      "loss: 0.2561, acc: 0.8828, test_acc: 0.7598, f1: 0.6491\n",
      "loss: 0.2713, acc: 0.8844, test_acc: 0.7464, f1: 0.6463\n",
      "loss: 0.5115, acc: 0.8810, test_acc: 0.7580, f1: 0.6554\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  15\n",
      "loss: 0.3126, acc: 0.8672, test_acc: 0.7527, f1: 0.6296\n",
      "loss: 0.2050, acc: 0.8945, test_acc: 0.7562, f1: 0.6653\n",
      "loss: 0.2383, acc: 0.8958, test_acc: 0.7643, f1: 0.6400\n",
      "loss: 0.3182, acc: 0.8926, test_acc: 0.7357, f1: 0.6325\n",
      "loss: 0.2551, acc: 0.8938, test_acc: 0.7571, f1: 0.6204\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  16\n",
      "loss: 0.1299, acc: 0.9453, test_acc: 0.7643, f1: 0.6675\n",
      "loss: 0.2506, acc: 0.9336, test_acc: 0.7661, f1: 0.6390\n",
      "loss: 0.2494, acc: 0.9323, test_acc: 0.7679, f1: 0.6659\n",
      "loss: 0.2543, acc: 0.9277, test_acc: 0.7607, f1: 0.6366\n",
      "loss: 0.2359, acc: 0.9203, test_acc: 0.7616, f1: 0.6566\n",
      "loss: 0.2564, acc: 0.9193, test_acc: 0.7518, f1: 0.6271\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  17\n",
      "loss: 0.2152, acc: 0.9062, test_acc: 0.7696, f1: 0.6703\n",
      "loss: 0.1607, acc: 0.9297, test_acc: 0.7491, f1: 0.6598\n",
      "loss: 0.2790, acc: 0.9167, test_acc: 0.7616, f1: 0.6357\n",
      "loss: 0.2373, acc: 0.9160, test_acc: 0.7625, f1: 0.6404\n",
      "loss: 0.2484, acc: 0.9125, test_acc: 0.7607, f1: 0.6403\n",
      "loss: 0.1833, acc: 0.9180, test_acc: 0.7580, f1: 0.6396\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  18\n",
      "loss: 0.2075, acc: 0.9141, test_acc: 0.7554, f1: 0.6342\n",
      "loss: 0.1879, acc: 0.9180, test_acc: 0.7571, f1: 0.6524\n",
      "loss: 0.1986, acc: 0.9323, test_acc: 0.7652, f1: 0.6405\n",
      "loss: 0.2047, acc: 0.9316, test_acc: 0.7634, f1: 0.6504\n",
      "loss: 0.2341, acc: 0.9297, test_acc: 0.7562, f1: 0.6458\n",
      "loss: 0.2658, acc: 0.9245, test_acc: 0.7589, f1: 0.6325\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  19\n",
      "loss: 0.3121, acc: 0.8750, test_acc: 0.7321, f1: 0.6348\n",
      "loss: 0.2741, acc: 0.8984, test_acc: 0.7509, f1: 0.6332\n",
      "loss: 0.1677, acc: 0.9141, test_acc: 0.7312, f1: 0.6031\n",
      "loss: 0.2591, acc: 0.9102, test_acc: 0.7607, f1: 0.6419\n",
      "loss: 0.2611, acc: 0.9125, test_acc: 0.7571, f1: 0.6396\n",
      "loss: 0.2813, acc: 0.9111, test_acc: 0.7634, f1: 0.6240\n",
      "max_test_acc: 0.7767857142857143     max_f1: 0.6776134588021012\n",
      "####################################################################################################\n",
      "repeat:  2\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n",
      "loss: 1.0975, acc: 0.3281, test_acc: 0.6152, f1: 0.2925\n",
      "loss: 1.0395, acc: 0.4414, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 1.0405, acc: 0.4844, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9699, acc: 0.5137, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9930, acc: 0.5344, test_acc: 0.6500, f1: 0.2626\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 0.9090, acc: 0.6797, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9778, acc: 0.6328, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9199, acc: 0.6250, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9690, acc: 0.6230, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9509, acc: 0.6156, test_acc: 0.6571, f1: 0.2929\n",
      "loss: 0.8496, acc: 0.6198, test_acc: 0.6661, f1: 0.3264\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      "loss: 0.8174, acc: 0.6484, test_acc: 0.6866, f1: 0.4144\n",
      "loss: 0.8665, acc: 0.6406, test_acc: 0.6839, f1: 0.3866\n",
      "loss: 0.7907, acc: 0.6432, test_acc: 0.6875, f1: 0.4512\n",
      "loss: 0.7779, acc: 0.6484, test_acc: 0.6893, f1: 0.4062\n",
      "loss: 0.7844, acc: 0.6547, test_acc: 0.7161, f1: 0.5353\n",
      "loss: 0.6365, acc: 0.6758, test_acc: 0.7009, f1: 0.4330\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 0.7730, acc: 0.6797, test_acc: 0.6937, f1: 0.4627\n",
      "loss: 0.7121, acc: 0.7070, test_acc: 0.7134, f1: 0.5395\n",
      "loss: 0.7589, acc: 0.6901, test_acc: 0.7125, f1: 0.4731\n",
      "loss: 0.7364, acc: 0.6992, test_acc: 0.7080, f1: 0.4676\n",
      "loss: 0.6213, acc: 0.7047, test_acc: 0.7196, f1: 0.4892\n",
      "loss: 0.7685, acc: 0.7018, test_acc: 0.7214, f1: 0.5043\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.6857, acc: 0.6797, test_acc: 0.7259, f1: 0.5859\n",
      "loss: 0.7248, acc: 0.7070, test_acc: 0.7196, f1: 0.5619\n",
      "loss: 0.6932, acc: 0.6953, test_acc: 0.7214, f1: 0.4853\n",
      "loss: 0.6695, acc: 0.7012, test_acc: 0.7214, f1: 0.5096\n",
      "loss: 0.6568, acc: 0.7125, test_acc: 0.7330, f1: 0.5387\n",
      "loss: 0.4523, acc: 0.7184, test_acc: 0.7241, f1: 0.5579\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.5437, acc: 0.7812, test_acc: 0.7286, f1: 0.5324\n",
      "loss: 0.6802, acc: 0.7539, test_acc: 0.7429, f1: 0.5775\n",
      "loss: 0.6217, acc: 0.7604, test_acc: 0.7500, f1: 0.5960\n",
      "loss: 0.6322, acc: 0.7520, test_acc: 0.7348, f1: 0.5458\n",
      "loss: 0.6405, acc: 0.7438, test_acc: 0.7330, f1: 0.5308\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.6857, acc: 0.7422, test_acc: 0.7411, f1: 0.5740\n",
      "loss: 0.7073, acc: 0.7266, test_acc: 0.7482, f1: 0.5962\n",
      "loss: 0.5101, acc: 0.7396, test_acc: 0.7402, f1: 0.5659\n",
      "loss: 0.6059, acc: 0.7402, test_acc: 0.7482, f1: 0.5930\n",
      "loss: 0.5473, acc: 0.7531, test_acc: 0.7482, f1: 0.5757\n",
      "loss: 0.5820, acc: 0.7591, test_acc: 0.7527, f1: 0.6089\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n",
      "loss: 0.4833, acc: 0.8203, test_acc: 0.7571, f1: 0.6170\n",
      "loss: 0.7688, acc: 0.7344, test_acc: 0.7527, f1: 0.6041\n",
      "loss: 0.5734, acc: 0.7370, test_acc: 0.7741, f1: 0.6567\n",
      "loss: 0.3590, acc: 0.7715, test_acc: 0.7580, f1: 0.5886\n",
      "loss: 0.6113, acc: 0.7688, test_acc: 0.7688, f1: 0.6384\n",
      "loss: 0.6400, acc: 0.7695, test_acc: 0.7741, f1: 0.6324\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  8\n",
      "loss: 0.5163, acc: 0.7578, test_acc: 0.7616, f1: 0.5958\n",
      "loss: 0.4395, acc: 0.7891, test_acc: 0.7670, f1: 0.6300\n",
      "loss: 0.5752, acc: 0.7734, test_acc: 0.7589, f1: 0.6124\n",
      "loss: 0.5783, acc: 0.7715, test_acc: 0.7616, f1: 0.6200\n",
      "loss: 0.5706, acc: 0.7703, test_acc: 0.7643, f1: 0.6233\n",
      "loss: 0.5341, acc: 0.7773, test_acc: 0.7562, f1: 0.6190\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  9\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7839\n",
      "loss: 0.4532, acc: 0.8047, test_acc: 0.7839, f1: 0.6705\n",
      "loss: 0.5146, acc: 0.7969, test_acc: 0.7661, f1: 0.6500\n",
      "loss: 0.3966, acc: 0.8203, test_acc: 0.7625, f1: 0.6330\n",
      "loss: 0.5689, acc: 0.8105, test_acc: 0.7545, f1: 0.6083\n",
      "loss: 0.4360, acc: 0.8141, test_acc: 0.7634, f1: 0.6157\n",
      "loss: 0.8677, acc: 0.8117, test_acc: 0.7562, f1: 0.6549\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  10\n",
      "loss: 0.4834, acc: 0.7734, test_acc: 0.7750, f1: 0.6585\n",
      "loss: 0.4567, acc: 0.7773, test_acc: 0.7607, f1: 0.6322\n",
      "loss: 0.3520, acc: 0.8047, test_acc: 0.7661, f1: 0.6474\n",
      "loss: 0.3162, acc: 0.8320, test_acc: 0.7679, f1: 0.6324\n",
      "loss: 0.4649, acc: 0.8250, test_acc: 0.7741, f1: 0.6478\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  11\n",
      "loss: 0.3436, acc: 0.8672, test_acc: 0.7714, f1: 0.6422\n",
      "loss: 0.4204, acc: 0.8594, test_acc: 0.7652, f1: 0.6699\n",
      "loss: 0.4361, acc: 0.8411, test_acc: 0.7714, f1: 0.6296\n",
      "loss: 0.4757, acc: 0.8203, test_acc: 0.7705, f1: 0.6670\n",
      "loss: 0.4826, acc: 0.8094, test_acc: 0.7804, f1: 0.6618\n",
      "loss: 0.3955, acc: 0.8125, test_acc: 0.7714, f1: 0.6426\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  12\n",
      "loss: 0.3928, acc: 0.8438, test_acc: 0.7777, f1: 0.6647\n",
      "loss: 0.4371, acc: 0.8242, test_acc: 0.7661, f1: 0.6726\n",
      "loss: 0.2824, acc: 0.8516, test_acc: 0.7714, f1: 0.6380\n",
      "loss: 0.3876, acc: 0.8496, test_acc: 0.7777, f1: 0.6626\n",
      "loss: 0.3600, acc: 0.8516, test_acc: 0.7804, f1: 0.6698\n",
      "loss: 0.3573, acc: 0.8477, test_acc: 0.7750, f1: 0.6738\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  13\n",
      "loss: 0.3737, acc: 0.8438, test_acc: 0.7750, f1: 0.6542\n",
      "loss: 0.1996, acc: 0.8633, test_acc: 0.7643, f1: 0.6620\n",
      "loss: 0.3878, acc: 0.8594, test_acc: 0.7705, f1: 0.6620\n",
      "loss: 0.3367, acc: 0.8555, test_acc: 0.7607, f1: 0.6536\n",
      "loss: 0.3345, acc: 0.8531, test_acc: 0.7723, f1: 0.6601\n",
      "loss: 0.3399, acc: 0.8503, test_acc: 0.7696, f1: 0.6690\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  14\n",
      "loss: 0.2719, acc: 0.9297, test_acc: 0.7804, f1: 0.6723\n",
      "loss: 0.3381, acc: 0.8750, test_acc: 0.7634, f1: 0.6466\n",
      "loss: 0.2779, acc: 0.8724, test_acc: 0.7795, f1: 0.6928\n",
      "loss: 0.3960, acc: 0.8633, test_acc: 0.7643, f1: 0.6455\n",
      "loss: 0.2687, acc: 0.8703, test_acc: 0.7821, f1: 0.6769\n",
      "loss: 0.1086, acc: 0.8735, test_acc: 0.7795, f1: 0.6658\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  15\n",
      "loss: 0.2601, acc: 0.9141, test_acc: 0.7759, f1: 0.6571\n",
      "loss: 0.1931, acc: 0.9180, test_acc: 0.7786, f1: 0.6800\n",
      "loss: 0.2948, acc: 0.9141, test_acc: 0.7589, f1: 0.6661\n",
      "loss: 0.3513, acc: 0.8965, test_acc: 0.7670, f1: 0.6489\n",
      "loss: 0.2366, acc: 0.9000, test_acc: 0.7679, f1: 0.6782\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  16\n",
      "loss: 0.2172, acc: 0.9219, test_acc: 0.7705, f1: 0.6444\n",
      "loss: 0.2752, acc: 0.9062, test_acc: 0.7812, f1: 0.6879\n",
      "loss: 0.3330, acc: 0.8984, test_acc: 0.7750, f1: 0.6782\n",
      "loss: 0.3211, acc: 0.8867, test_acc: 0.7768, f1: 0.6631\n",
      "loss: 0.3474, acc: 0.8875, test_acc: 0.7616, f1: 0.6638\n",
      "loss: 0.2170, acc: 0.8906, test_acc: 0.7696, f1: 0.6614\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  17\n",
      "loss: 0.1956, acc: 0.9219, test_acc: 0.7598, f1: 0.6621\n",
      "loss: 0.1748, acc: 0.9297, test_acc: 0.7536, f1: 0.6460\n",
      "loss: 0.2644, acc: 0.9141, test_acc: 0.7598, f1: 0.6508\n",
      "loss: 0.3341, acc: 0.8984, test_acc: 0.7661, f1: 0.6602\n",
      "loss: 0.3055, acc: 0.9016, test_acc: 0.7705, f1: 0.6663\n",
      "loss: 0.3215, acc: 0.8997, test_acc: 0.7598, f1: 0.6643\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  18\n",
      "loss: 0.1762, acc: 0.9453, test_acc: 0.7562, f1: 0.6495\n",
      "loss: 0.2658, acc: 0.9102, test_acc: 0.7670, f1: 0.6596\n",
      "loss: 0.2391, acc: 0.9062, test_acc: 0.7634, f1: 0.6639\n",
      "loss: 0.2715, acc: 0.8984, test_acc: 0.7634, f1: 0.6467\n",
      "loss: 0.2202, acc: 0.9031, test_acc: 0.7509, f1: 0.6597\n",
      "loss: 0.1990, acc: 0.9089, test_acc: 0.7732, f1: 0.6607\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  19\n",
      "loss: 0.3305, acc: 0.8906, test_acc: 0.7759, f1: 0.6656\n",
      "loss: 0.1960, acc: 0.9102, test_acc: 0.7509, f1: 0.6568\n",
      "loss: 0.2522, acc: 0.9167, test_acc: 0.7679, f1: 0.6470\n",
      "loss: 0.1434, acc: 0.9258, test_acc: 0.7616, f1: 0.6585\n",
      "loss: 0.2389, acc: 0.9219, test_acc: 0.7509, f1: 0.6575\n",
      "loss: 0.7889, acc: 0.9157, test_acc: 0.7589, f1: 0.6617\n",
      "max_test_acc: 0.7839285714285714     max_f1: 0.6928105594772261\n",
      "####################################################################################################\n",
      "repeat:  3\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n",
      "loss: 1.1036, acc: 0.1875, test_acc: 0.5357, f1: 0.3292\n",
      "loss: 1.0365, acc: 0.3633, test_acc: 0.6473, f1: 0.2748\n",
      "loss: 0.9314, acc: 0.4635, test_acc: 0.6482, f1: 0.2625\n",
      "loss: 1.0637, acc: 0.4805, test_acc: 0.6482, f1: 0.2623\n",
      "loss: 1.0346, acc: 0.5031, test_acc: 0.6491, f1: 0.2625\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 1.0240, acc: 0.5469, test_acc: 0.6500, f1: 0.2661\n",
      "loss: 0.9038, acc: 0.5781, test_acc: 0.6500, f1: 0.2628\n",
      "loss: 0.8171, acc: 0.6120, test_acc: 0.6500, f1: 0.2626\n",
      "loss: 0.9633, acc: 0.5977, test_acc: 0.6536, f1: 0.2800\n",
      "loss: 0.8525, acc: 0.6047, test_acc: 0.6804, f1: 0.4336\n",
      "loss: 0.8162, acc: 0.6094, test_acc: 0.6768, f1: 0.3610\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      "loss: 0.9285, acc: 0.5625, test_acc: 0.7036, f1: 0.4994\n",
      "loss: 0.7848, acc: 0.6016, test_acc: 0.7009, f1: 0.4870\n",
      "loss: 0.7665, acc: 0.6224, test_acc: 0.7143, f1: 0.5043\n",
      "loss: 0.8146, acc: 0.6270, test_acc: 0.6991, f1: 0.4488\n",
      "loss: 0.7482, acc: 0.6438, test_acc: 0.7277, f1: 0.5657\n",
      "loss: 0.7867, acc: 0.6419, test_acc: 0.7304, f1: 0.5481\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 0.7799, acc: 0.6172, test_acc: 0.7170, f1: 0.4939\n",
      "loss: 0.6947, acc: 0.6641, test_acc: 0.7188, f1: 0.5418\n",
      "loss: 0.6885, acc: 0.7005, test_acc: 0.7179, f1: 0.5072\n",
      "loss: 0.6679, acc: 0.7012, test_acc: 0.7446, f1: 0.5982\n",
      "loss: 0.7256, acc: 0.6984, test_acc: 0.7402, f1: 0.5631\n",
      "loss: 0.7625, acc: 0.7031, test_acc: 0.7402, f1: 0.5805\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.7371, acc: 0.6641, test_acc: 0.7464, f1: 0.6046\n",
      "loss: 0.5959, acc: 0.7148, test_acc: 0.7455, f1: 0.5842\n",
      "loss: 0.6690, acc: 0.7214, test_acc: 0.7321, f1: 0.5592\n",
      "loss: 0.6511, acc: 0.7266, test_acc: 0.7348, f1: 0.5628\n",
      "loss: 0.5958, acc: 0.7359, test_acc: 0.7402, f1: 0.5988\n",
      "loss: 0.9723, acc: 0.7319, test_acc: 0.7562, f1: 0.6163\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.6494, acc: 0.7266, test_acc: 0.7589, f1: 0.5927\n",
      "loss: 0.6222, acc: 0.7305, test_acc: 0.7464, f1: 0.5564\n",
      "loss: 0.6200, acc: 0.7318, test_acc: 0.7607, f1: 0.6316\n",
      "loss: 0.7366, acc: 0.7188, test_acc: 0.7580, f1: 0.6186\n",
      "loss: 0.6720, acc: 0.7172, test_acc: 0.7554, f1: 0.5928\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.5370, acc: 0.8047, test_acc: 0.7696, f1: 0.6570\n",
      "loss: 0.5869, acc: 0.7930, test_acc: 0.7643, f1: 0.6186\n",
      "loss: 0.6627, acc: 0.7526, test_acc: 0.7643, f1: 0.6159\n",
      "loss: 0.5475, acc: 0.7617, test_acc: 0.7696, f1: 0.6465\n",
      "loss: 0.5801, acc: 0.7672, test_acc: 0.7723, f1: 0.6545\n",
      "loss: 0.5144, acc: 0.7708, test_acc: 0.7554, f1: 0.5943\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n",
      "loss: 0.4927, acc: 0.8047, test_acc: 0.7589, f1: 0.6497\n",
      "loss: 0.5745, acc: 0.7812, test_acc: 0.7634, f1: 0.6219\n",
      "loss: 0.5966, acc: 0.7734, test_acc: 0.7688, f1: 0.6251\n",
      "loss: 0.5393, acc: 0.7793, test_acc: 0.7598, f1: 0.6290\n",
      "loss: 0.6535, acc: 0.7703, test_acc: 0.7643, f1: 0.6158\n",
      "loss: 0.5170, acc: 0.7721, test_acc: 0.7688, f1: 0.6573\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  8\n",
      "loss: 0.4895, acc: 0.8125, test_acc: 0.7616, f1: 0.6267\n",
      "loss: 0.5243, acc: 0.7852, test_acc: 0.7643, f1: 0.6355\n",
      "loss: 0.4666, acc: 0.7995, test_acc: 0.7732, f1: 0.6597\n",
      "loss: 0.5844, acc: 0.7695, test_acc: 0.7777, f1: 0.6655\n",
      "loss: 0.4783, acc: 0.7688, test_acc: 0.7616, f1: 0.6423\n",
      "loss: 0.4639, acc: 0.7799, test_acc: 0.7652, f1: 0.6116\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  9\n",
      "loss: 0.5820, acc: 0.7344, test_acc: 0.7607, f1: 0.6670\n",
      "loss: 0.5533, acc: 0.7578, test_acc: 0.7634, f1: 0.6062\n",
      "loss: 0.4990, acc: 0.7708, test_acc: 0.7759, f1: 0.6637\n",
      "loss: 0.4920, acc: 0.7793, test_acc: 0.7705, f1: 0.6420\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7857\n",
      "loss: 0.4616, acc: 0.7875, test_acc: 0.7857, f1: 0.6852\n",
      "loss: 0.9170, acc: 0.7846, test_acc: 0.7777, f1: 0.6626\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  10\n",
      ">> saved: state_dict/aoa_restaurant_acc0.7893\n",
      "loss: 0.4773, acc: 0.8359, test_acc: 0.7893, f1: 0.6716\n",
      "loss: 0.4451, acc: 0.8438, test_acc: 0.7598, f1: 0.6460\n",
      "loss: 0.4061, acc: 0.8385, test_acc: 0.7795, f1: 0.6702\n",
      "loss: 0.4754, acc: 0.8340, test_acc: 0.7714, f1: 0.6613\n",
      "loss: 0.4656, acc: 0.8297, test_acc: 0.7821, f1: 0.6636\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  11\n",
      "loss: 0.3898, acc: 0.8125, test_acc: 0.7688, f1: 0.6449\n",
      "loss: 0.4279, acc: 0.7969, test_acc: 0.7679, f1: 0.6358\n",
      "loss: 0.3784, acc: 0.8177, test_acc: 0.7768, f1: 0.6762\n",
      "loss: 0.5767, acc: 0.7891, test_acc: 0.7795, f1: 0.6665\n",
      "loss: 0.3845, acc: 0.7984, test_acc: 0.7804, f1: 0.6843\n",
      "loss: 0.4253, acc: 0.8008, test_acc: 0.7723, f1: 0.6652\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  12\n",
      "loss: 0.3687, acc: 0.8516, test_acc: 0.7714, f1: 0.6649\n",
      "loss: 0.3401, acc: 0.8594, test_acc: 0.7688, f1: 0.6551\n",
      "loss: 0.4020, acc: 0.8490, test_acc: 0.7786, f1: 0.6601\n",
      "loss: 0.3899, acc: 0.8477, test_acc: 0.7759, f1: 0.6730\n",
      "loss: 0.3623, acc: 0.8469, test_acc: 0.7723, f1: 0.6580\n",
      "loss: 0.4705, acc: 0.8359, test_acc: 0.7598, f1: 0.6437\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  13\n",
      "loss: 0.4035, acc: 0.8438, test_acc: 0.7688, f1: 0.6607\n",
      "loss: 0.4669, acc: 0.8438, test_acc: 0.7634, f1: 0.6297\n",
      "loss: 0.3325, acc: 0.8568, test_acc: 0.7750, f1: 0.6769\n",
      "loss: 0.4147, acc: 0.8516, test_acc: 0.7652, f1: 0.6252\n",
      "loss: 0.3928, acc: 0.8469, test_acc: 0.7688, f1: 0.6511\n",
      "loss: 0.4235, acc: 0.8438, test_acc: 0.7589, f1: 0.6562\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  14\n",
      "loss: 0.3125, acc: 0.8594, test_acc: 0.7607, f1: 0.6499\n",
      "loss: 0.3435, acc: 0.8594, test_acc: 0.7670, f1: 0.6365\n",
      "loss: 0.2434, acc: 0.8698, test_acc: 0.7741, f1: 0.6669\n",
      "loss: 0.3793, acc: 0.8633, test_acc: 0.7750, f1: 0.6755\n",
      "loss: 0.3743, acc: 0.8641, test_acc: 0.7652, f1: 0.6493\n",
      "loss: 0.4144, acc: 0.8660, test_acc: 0.7598, f1: 0.6442\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  15\n",
      "loss: 0.4132, acc: 0.8281, test_acc: 0.7509, f1: 0.6369\n",
      "loss: 0.3616, acc: 0.8438, test_acc: 0.7580, f1: 0.6310\n",
      "loss: 0.2280, acc: 0.8620, test_acc: 0.7661, f1: 0.6265\n",
      "loss: 0.3627, acc: 0.8652, test_acc: 0.7643, f1: 0.6602\n",
      "loss: 0.2543, acc: 0.8719, test_acc: 0.7777, f1: 0.6656\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  16\n",
      "loss: 0.3056, acc: 0.8594, test_acc: 0.7759, f1: 0.6633\n",
      "loss: 0.2686, acc: 0.8711, test_acc: 0.7768, f1: 0.6747\n",
      "loss: 0.2347, acc: 0.8802, test_acc: 0.7821, f1: 0.6723\n",
      "loss: 0.2203, acc: 0.8926, test_acc: 0.7714, f1: 0.6608\n",
      "loss: 0.2246, acc: 0.8984, test_acc: 0.7750, f1: 0.6570\n",
      "loss: 0.2725, acc: 0.8997, test_acc: 0.7688, f1: 0.6563\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  17\n",
      "loss: 0.2947, acc: 0.8984, test_acc: 0.7589, f1: 0.6539\n",
      "loss: 0.1785, acc: 0.9102, test_acc: 0.7652, f1: 0.6563\n",
      "loss: 0.1902, acc: 0.9219, test_acc: 0.7714, f1: 0.6489\n",
      "loss: 0.1907, acc: 0.9219, test_acc: 0.7625, f1: 0.6619\n",
      "loss: 0.2628, acc: 0.9203, test_acc: 0.7768, f1: 0.6602\n",
      "loss: 0.2785, acc: 0.9089, test_acc: 0.7446, f1: 0.6495\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  18\n",
      "loss: 0.2547, acc: 0.8906, test_acc: 0.7652, f1: 0.6297\n",
      "loss: 0.3290, acc: 0.8906, test_acc: 0.7696, f1: 0.6769\n",
      "loss: 0.2864, acc: 0.8906, test_acc: 0.7714, f1: 0.6562\n",
      "loss: 0.3276, acc: 0.8809, test_acc: 0.7723, f1: 0.6554\n",
      "loss: 0.3206, acc: 0.8766, test_acc: 0.7616, f1: 0.6427\n",
      "loss: 0.3331, acc: 0.8711, test_acc: 0.7732, f1: 0.6633\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  19\n",
      "loss: 0.2802, acc: 0.8984, test_acc: 0.7750, f1: 0.6622\n",
      "loss: 0.2515, acc: 0.9023, test_acc: 0.7866, f1: 0.6898\n",
      "loss: 0.1882, acc: 0.9115, test_acc: 0.7741, f1: 0.6669\n",
      "loss: 0.1718, acc: 0.9199, test_acc: 0.7714, f1: 0.6594\n",
      "loss: 0.2923, acc: 0.9187, test_acc: 0.7705, f1: 0.6612\n",
      "loss: 0.2412, acc: 0.9202, test_acc: 0.7723, f1: 0.6817\n",
      "max_test_acc: 0.7892857142857143     max_f1: 0.6898395676266174\n",
      "####################################################################################################\n",
      "repeat:  4\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  0\n",
      "loss: 1.1574, acc: 0.2656, test_acc: 0.3366, f1: 0.2372\n",
      "loss: 1.0985, acc: 0.3906, test_acc: 0.6232, f1: 0.3111\n",
      "loss: 1.0747, acc: 0.4531, test_acc: 0.6420, f1: 0.2897\n",
      "loss: 1.0583, acc: 0.4688, test_acc: 0.6491, f1: 0.2871\n",
      "loss: 0.9547, acc: 0.5062, test_acc: 0.6509, f1: 0.2877\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  1\n",
      "loss: 1.0347, acc: 0.4844, test_acc: 0.6518, f1: 0.2987\n",
      "loss: 0.9214, acc: 0.5352, test_acc: 0.6562, f1: 0.2953\n",
      "loss: 0.9466, acc: 0.5599, test_acc: 0.6545, f1: 0.2831\n",
      "loss: 0.8790, acc: 0.5684, test_acc: 0.6607, f1: 0.3078\n",
      "loss: 0.9126, acc: 0.5922, test_acc: 0.6679, f1: 0.3362\n",
      "loss: 0.8608, acc: 0.5964, test_acc: 0.6964, f1: 0.4136\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  2\n",
      "loss: 0.8186, acc: 0.6484, test_acc: 0.6732, f1: 0.4443\n",
      "loss: 0.8361, acc: 0.6445, test_acc: 0.6955, f1: 0.4201\n",
      "loss: 0.8697, acc: 0.6380, test_acc: 0.7027, f1: 0.4862\n",
      "loss: 0.8137, acc: 0.6465, test_acc: 0.6937, f1: 0.4112\n",
      "loss: 0.8344, acc: 0.6500, test_acc: 0.6964, f1: 0.4681\n",
      "loss: 0.7443, acc: 0.6549, test_acc: 0.7089, f1: 0.4476\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  3\n",
      "loss: 0.7951, acc: 0.6719, test_acc: 0.7071, f1: 0.4904\n",
      "loss: 0.7542, acc: 0.6914, test_acc: 0.7259, f1: 0.5137\n",
      "loss: 0.7225, acc: 0.6979, test_acc: 0.7027, f1: 0.4802\n",
      "loss: 0.7136, acc: 0.7051, test_acc: 0.7161, f1: 0.4904\n",
      "loss: 0.6563, acc: 0.7078, test_acc: 0.7250, f1: 0.5766\n",
      "loss: 0.7494, acc: 0.7070, test_acc: 0.7188, f1: 0.5171\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  4\n",
      "loss: 0.6505, acc: 0.7500, test_acc: 0.7259, f1: 0.5428\n",
      "loss: 0.7373, acc: 0.7188, test_acc: 0.7304, f1: 0.5308\n",
      "loss: 0.6893, acc: 0.7188, test_acc: 0.7357, f1: 0.5679\n",
      "loss: 0.6011, acc: 0.7266, test_acc: 0.7179, f1: 0.5006\n",
      "loss: 0.6711, acc: 0.7281, test_acc: 0.7375, f1: 0.5413\n",
      "loss: 0.6776, acc: 0.7304, test_acc: 0.7357, f1: 0.5392\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  5\n",
      "loss: 0.6075, acc: 0.7969, test_acc: 0.7295, f1: 0.5798\n",
      "loss: 0.6753, acc: 0.7539, test_acc: 0.7339, f1: 0.5572\n",
      "loss: 0.7387, acc: 0.7370, test_acc: 0.7393, f1: 0.5986\n",
      "loss: 0.6242, acc: 0.7305, test_acc: 0.7312, f1: 0.5299\n",
      "loss: 0.7081, acc: 0.7172, test_acc: 0.7357, f1: 0.5911\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  6\n",
      "loss: 0.5485, acc: 0.8125, test_acc: 0.7607, f1: 0.6303\n",
      "loss: 0.6255, acc: 0.7773, test_acc: 0.7616, f1: 0.6357\n",
      "loss: 0.6306, acc: 0.7578, test_acc: 0.7491, f1: 0.6024\n",
      "loss: 0.6330, acc: 0.7480, test_acc: 0.7357, f1: 0.5363\n",
      "loss: 0.5540, acc: 0.7531, test_acc: 0.7438, f1: 0.5976\n",
      "loss: 0.6360, acc: 0.7461, test_acc: 0.7482, f1: 0.5930\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  7\n",
      "loss: 0.7151, acc: 0.6875, test_acc: 0.7429, f1: 0.5984\n",
      "loss: 0.5197, acc: 0.7539, test_acc: 0.7455, f1: 0.5636\n",
      "loss: 0.5714, acc: 0.7526, test_acc: 0.7589, f1: 0.6080\n",
      "loss: 0.5785, acc: 0.7578, test_acc: 0.7402, f1: 0.5789\n",
      "loss: 0.5415, acc: 0.7609, test_acc: 0.7536, f1: 0.5888\n",
      "loss: 0.5848, acc: 0.7630, test_acc: 0.7509, f1: 0.6135\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  8\n",
      "loss: 0.4971, acc: 0.7891, test_acc: 0.7536, f1: 0.6029\n",
      "loss: 0.5801, acc: 0.8047, test_acc: 0.7402, f1: 0.5912\n",
      "loss: 0.5470, acc: 0.7891, test_acc: 0.7518, f1: 0.5992\n",
      "loss: 0.6418, acc: 0.7812, test_acc: 0.7527, f1: 0.6450\n",
      "loss: 0.4174, acc: 0.7844, test_acc: 0.7562, f1: 0.6202\n",
      "loss: 0.5411, acc: 0.7852, test_acc: 0.7616, f1: 0.6116\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  9\n",
      "loss: 0.4844, acc: 0.7891, test_acc: 0.7705, f1: 0.6610\n",
      "loss: 0.6077, acc: 0.7695, test_acc: 0.7545, f1: 0.5965\n",
      "loss: 0.4694, acc: 0.7708, test_acc: 0.7661, f1: 0.6188\n",
      "loss: 0.4018, acc: 0.7949, test_acc: 0.7705, f1: 0.6298\n",
      "loss: 0.5806, acc: 0.7953, test_acc: 0.7705, f1: 0.6539\n",
      "loss: 0.4403, acc: 0.7982, test_acc: 0.7607, f1: 0.6322\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  10\n",
      "loss: 0.5285, acc: 0.7891, test_acc: 0.7571, f1: 0.6203\n",
      "loss: 0.3285, acc: 0.8359, test_acc: 0.7589, f1: 0.6197\n",
      "loss: 0.4673, acc: 0.8281, test_acc: 0.7661, f1: 0.6419\n",
      "loss: 0.4401, acc: 0.8242, test_acc: 0.7723, f1: 0.6544\n",
      "loss: 0.5493, acc: 0.8141, test_acc: 0.7723, f1: 0.6638\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  11\n",
      "loss: 0.4136, acc: 0.8281, test_acc: 0.7446, f1: 0.5844\n",
      "loss: 0.3446, acc: 0.8359, test_acc: 0.7732, f1: 0.6573\n",
      "loss: 0.4035, acc: 0.8359, test_acc: 0.7696, f1: 0.6601\n",
      "loss: 0.3721, acc: 0.8457, test_acc: 0.7438, f1: 0.5597\n",
      "loss: 0.3203, acc: 0.8531, test_acc: 0.7652, f1: 0.6643\n",
      "loss: 0.3527, acc: 0.8607, test_acc: 0.7723, f1: 0.6463\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  12\n",
      "loss: 0.2407, acc: 0.9297, test_acc: 0.7580, f1: 0.6425\n",
      "loss: 0.3901, acc: 0.8828, test_acc: 0.7705, f1: 0.6528\n",
      "loss: 0.3445, acc: 0.8776, test_acc: 0.7759, f1: 0.6618\n",
      "loss: 0.3840, acc: 0.8691, test_acc: 0.7741, f1: 0.6594\n",
      "loss: 0.3594, acc: 0.8625, test_acc: 0.7679, f1: 0.6571\n",
      "loss: 0.3314, acc: 0.8620, test_acc: 0.7598, f1: 0.6362\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  13\n",
      "loss: 0.3809, acc: 0.8438, test_acc: 0.7616, f1: 0.6414\n",
      "loss: 0.2779, acc: 0.8828, test_acc: 0.7679, f1: 0.6447\n",
      "loss: 0.2709, acc: 0.8802, test_acc: 0.7616, f1: 0.6431\n",
      "loss: 0.4011, acc: 0.8613, test_acc: 0.7545, f1: 0.6154\n",
      "loss: 0.3621, acc: 0.8656, test_acc: 0.7554, f1: 0.6367\n",
      "loss: 0.4470, acc: 0.8633, test_acc: 0.7714, f1: 0.6548\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  14\n",
      "loss: 0.4224, acc: 0.7969, test_acc: 0.7545, f1: 0.6166\n",
      "loss: 0.3485, acc: 0.8320, test_acc: 0.7580, f1: 0.6449\n",
      "loss: 0.3312, acc: 0.8490, test_acc: 0.7625, f1: 0.6271\n",
      "loss: 0.2551, acc: 0.8613, test_acc: 0.7759, f1: 0.6607\n",
      "loss: 0.3351, acc: 0.8641, test_acc: 0.7500, f1: 0.6593\n",
      "loss: 0.5354, acc: 0.8584, test_acc: 0.7661, f1: 0.6469\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  15\n",
      "loss: 0.3395, acc: 0.8672, test_acc: 0.7625, f1: 0.6632\n",
      "loss: 0.3520, acc: 0.8672, test_acc: 0.7741, f1: 0.6550\n",
      "loss: 0.3843, acc: 0.8516, test_acc: 0.7545, f1: 0.6629\n",
      "loss: 0.4168, acc: 0.8477, test_acc: 0.7545, f1: 0.6039\n",
      "loss: 0.3016, acc: 0.8531, test_acc: 0.7607, f1: 0.6528\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  16\n",
      "loss: 0.3490, acc: 0.8828, test_acc: 0.7420, f1: 0.6368\n",
      "loss: 0.3232, acc: 0.8711, test_acc: 0.7607, f1: 0.6434\n",
      "loss: 0.3096, acc: 0.8620, test_acc: 0.7464, f1: 0.6207\n",
      "loss: 0.2491, acc: 0.8750, test_acc: 0.7580, f1: 0.6428\n",
      "loss: 0.2290, acc: 0.8859, test_acc: 0.7643, f1: 0.6485\n",
      "loss: 0.2840, acc: 0.8880, test_acc: 0.7661, f1: 0.6480\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  17\n",
      "loss: 0.2419, acc: 0.8672, test_acc: 0.7562, f1: 0.6448\n",
      "loss: 0.1704, acc: 0.9023, test_acc: 0.7616, f1: 0.6539\n",
      "loss: 0.2283, acc: 0.9010, test_acc: 0.7616, f1: 0.6397\n",
      "loss: 0.2355, acc: 0.9043, test_acc: 0.7455, f1: 0.6304\n",
      "loss: 0.2758, acc: 0.8969, test_acc: 0.7643, f1: 0.6326\n",
      "loss: 0.2121, acc: 0.9036, test_acc: 0.7625, f1: 0.6436\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  18\n",
      "loss: 0.2328, acc: 0.8984, test_acc: 0.7312, f1: 0.6312\n",
      "loss: 0.1946, acc: 0.9102, test_acc: 0.7518, f1: 0.6003\n",
      "loss: 0.1589, acc: 0.9167, test_acc: 0.7625, f1: 0.6558\n",
      "loss: 0.2611, acc: 0.9121, test_acc: 0.7705, f1: 0.6437\n",
      "loss: 0.1561, acc: 0.9203, test_acc: 0.7625, f1: 0.6606\n",
      "loss: 0.1421, acc: 0.9258, test_acc: 0.7571, f1: 0.6485\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch:  19\n",
      "loss: 0.1872, acc: 0.9297, test_acc: 0.7670, f1: 0.6290\n",
      "loss: 0.1845, acc: 0.9297, test_acc: 0.7473, f1: 0.6394\n",
      "loss: 0.2771, acc: 0.9193, test_acc: 0.7616, f1: 0.6483\n",
      "loss: 0.2242, acc: 0.9199, test_acc: 0.7643, f1: 0.6195\n",
      "loss: 0.2557, acc: 0.9141, test_acc: 0.7625, f1: 0.6486\n",
      "loss: 0.3323, acc: 0.9127, test_acc: 0.7688, f1: 0.6488\n",
      "max_test_acc: 0.7758928571428572     max_f1: 0.6643200296797043\n",
      "####################################################################################################\n",
      "max_test_acc_overall: 0.7892857142857143\n",
      "max_f1_overall: 0.6928105594772261\n"
     ]
    }
   ],
   "source": [
    "ins.run(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
